{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with Reuters-21578 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See: https://kdd.ics.uci.edu/databases/reuters21578/README.txt for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.lancaster import *\n",
    "from nltk.stem.wordnet import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General constants (modify them according to you environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Numpy random seed\n",
    "random.seed(1000)\n",
    "\n",
    "# Newsline folder and format\n",
    "data_folder = './data'\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 200\n",
    "# Selected categories\n",
    "selected_categories = ['pl_usa']\n",
    "\n",
    "csv_df = pd.read_csv(data_folder + '/reutersCSV.csv', encoding='iso-8859-1', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare documents and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create category dataframe\n",
    "all_topics = [ re.sub('topic\\.', '', col) for col in csv_df.columns if col.startswith('topic.') ]\n",
    "print('\\n', 'num topics:', len(all_topics))\n",
    "pd.DataFrame(all_topics, columns=['topic name']).head(15)\n",
    "\n",
    "# Create category dataframe\n",
    "news_categories = DataFrame(data={'Name':all_topics, 'Type': 'Topics', 'Newslines':np.zeros(len(all_topics))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_frequencies(categories):\n",
    "    for category in categories:\n",
    "        idx = news_categories[news_categories.Name == category].index[0]\n",
    "        f = news_categories.get_value(idx, 'Newslines')\n",
    "        news_categories.set_value(idx, 'Newslines', f+1)\n",
    "    \n",
    "def to_category_vector(categories, target_categories):\n",
    "    vector = zeros(len(target_categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 20 categories (by number of newslines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_binarized = np.array(csv_df.values[:,3:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_list_from_binarized(binarized):\n",
    "    return [ topic for (has, topic) in zip(binarized, all_topics) if has == 1 ]\n",
    "\n",
    "topic_lists = [ topic_list_from_binarized(binarized) for binarized in topics_binarized ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = csv_df[[ 'pid', 'purpose', 'doc.title', 'doc.text' ]]\n",
    "df = df.assign(topic_list = topic_lists)\n",
    "df = df.assign(topics_binarized = topics_binarized.tolist())\n",
    "df = df.assign(num_topics = np.array([ len(lst) for lst in topic_lists ]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughly_even_topics = [\n",
    "    'money.fx',\n",
    "    'crude',\n",
    "    'grain',\n",
    "    'trade',\n",
    "    'interest',\n",
    "]\n",
    "all_topics = roughly_even_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roughly_even_topics_set = set(roughly_even_topics)\n",
    "df = df[df.apply(\n",
    "    lambda row: len(row['topic_list']) == 1 and set(row['topic_list']).issubset(roughly_even_topics_set),\n",
    "    axis = 1\n",
    ")]\n",
    "\n",
    "binarizer = MultiLabelBinarizer(classes = roughly_even_topics)\n",
    "\n",
    "topic_lists = df.apply(lambda row : row['topic_list'][0], axis = 1).values.reshape(len(df), 1)\n",
    "roughly_even_topics_binarized = binarizer.fit_transform(topic_lists)\n",
    "roughly_even_topics_binarized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize newsline documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['purpose'] == 'train']\n",
    "df_test = df[df['purpose'] == 'test']\n",
    "\n",
    "n_classes = len(all_topics)\n",
    "\n",
    "print('total docs', len(df))\n",
    "print('num train examples', len(df_train))\n",
    "print('num test examples', len(df_test))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(string):\n",
    "    return string.lower()\n",
    "\n",
    "def fix_lt(string):\n",
    "    # fix the HTML-escaped less-than sign\n",
    "    return re.sub(r'&lt;', '<', string)\n",
    "\n",
    "def remove_reuter(string):\n",
    "    # fix the HTML-escaped less-than sign\n",
    "    return re.sub(r'reuter$', '', string)\n",
    "\n",
    "def replace_non_alphanumeric_with_space(string):\n",
    "    # replace punctuation and different whitespace with space character\n",
    "    return re.sub(r'[^a-z0-9 ]', ' ', string)\n",
    "\n",
    "def strip_punctuation(string):\n",
    "    # remove punctuation\n",
    "    return re.sub(r'[^a-z0-9\\s]', ' ', string)\n",
    "\n",
    "def remove_stop_words(string, stop_words):\n",
    "    return ' '.join([ word for word in re.split(' ', string) if not word in stop_words ])\n",
    "\n",
    "def replace_numeric_with_literal(string):\n",
    "    return re.sub(r'([0-9]+ ?)+', '<num> ', string)\n",
    "\n",
    "def compact_whitespace(string):\n",
    "    return re.sub(r'\\s+', ' ', string)\n",
    "\n",
    "def stem(string, stemmer):\n",
    "    return ' '.join([ \n",
    "        stemmer.stem(word) if hasattr(stemmer, 'stem') else stemmer.lemmatize(word)\n",
    "        for word in re.split(' ', string) if not word in stop_words \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(string):\n",
    "    s = lower_case(string)\n",
    "    s = fix_lt(s)\n",
    "    s = remove_reuter(s)\n",
    "    s = strip_punctuation(s)\n",
    "    s = remove_stop_words(s, stop_words)\n",
    "    s = compact_whitespace(s)\n",
    "    s = replace_numeric_with_literal(s)\n",
    "    s = stem(s, stemmer)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_df(df):\n",
    "    return df.apply(lambda row : pre_process(row['doc.title'] + ' ' + row['doc.text']), axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_train = pre_process_df(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_test = pre_process_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_by_word(docs):\n",
    "    word_counts = {}\n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            if len(word) > 0:\n",
    "                if word not in word_counts:\n",
    "                    word_counts[word] = 0\n",
    "                word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "counts_by_word = get_counts_by_word(\n",
    "    df.apply(lambda row : row['doc.title'] + ' ' + row['doc.text'], axis = 1).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_word_pre_processed_train = get_counts_by_word(pre_processed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 3000\n",
    "pd.DataFrame(pd.Series(counts_by_word_pre_processed_train, name='count')).sort_values('count', ascending=False)[4:num_words].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.Series(counts_by_word_pre_processed_train, name='count')).sort_values('count', ascending=False).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(counts_by_word_pre_processed_train.items())\n",
    "top_words.sort(key = lambda it: it[1])\n",
    "top_words.reverse()\n",
    "top_words = [ word for word, count in top_words[:num_words] ]\n",
    "print(len(top_words))\n",
    "top_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rare_words_with_literal(string, top_words):\n",
    "    return ' '.join([ (word if word in top_words else '') for word in re.split(' ', string) ])\n",
    "\n",
    "pre_processed_train = [ \n",
    "    replace_rare_words_with_literal(doc, top_words) for doc in pre_processed_train\n",
    "]\n",
    "\n",
    "pre_processed_test = [ \n",
    "    replace_rare_words_with_literal(doc, top_words) for doc in pre_processed_test\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model\n",
    "### See: https://radimrehurek.com/gensim/models/word2vec.html and https://code.google.com/p/word2vec/ for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an existing Word2Vec model\n",
    "# w2v_model = Word2Vec.load(data_folder + 'reuters.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = pre_process_df(df)\n",
    "print(\"type\", type(all_sentences), \"shape\", all_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = [] \n",
    "for s in all_sentences: \n",
    "    allwords.append(s.split())\n",
    "\n",
    "print(len(allwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Gensim Word2Vec model\n",
    "w2v_model = Word2Vec(allwords, size=num_features, min_count=1, window=10, workers=cpu_count())\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(data_folder + '/reuters.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model[\"head\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(selected_categories)\n",
    "\n",
    "\n",
    "empty_word = zeros(num_features).astype(float32)\n",
    "\n",
    "def tokensToVec(documents):\n",
    "    X = zeros(shape=(len(documents), document_max_num_words, num_features)).astype(float32)\n",
    "    \n",
    "    for idx, document in enumerate(documents):\n",
    "        for jdx, word in enumerate(document.split()):\n",
    "            if jdx == document_max_num_words:\n",
    "                break\n",
    "            else:\n",
    "                if word in w2v_model:\n",
    "                    X[idx, jdx, :] = w2v_model[word]\n",
    "                else:\n",
    "                    X[idx, jdx, :] = empty_word\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokensToVec(pre_processed_train)\n",
    "X_test = tokensToVec(pre_processed_test)\n",
    "\n",
    "def to_category_vector(categories, target_categories):\n",
    "    vector = zeros(len(target_categories)).astype(float32)\n",
    "    \n",
    "    for i in range(len(target_categories)):\n",
    "        if target_categories[i] in categories:\n",
    "            vector[i] = 1.0\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def single_label(dft):\n",
    "    topics = dft.apply(lambda row : row['topic_list'][0], axis = 1).values\n",
    "    \n",
    "    result = []\n",
    "    for i in topics:\n",
    "        result.append(to_category_vector(i, roughly_even_topics))\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "Y_train = single_label(df_train)\n",
    "Y_test = single_label(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"document_max_num_words=\", document_max_num_words)\n",
    "print(\"num_features=\", num_features)\n",
    "\n",
    "num_categories = len(roughly_even_topics)\n",
    "print(\"num_categories \", num_categories)\n",
    "\n",
    "df_test[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_categories))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X_train, Y_train, batch_size=256, epochs=40, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(X_test, Y_test, batch_size=256)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(X_test, Y_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(t) :\n",
    "    maxId = np.argmax(t, axis=1)\n",
    "    binv = np.zeros((t.shape))\n",
    "    for idx, pos in enumerate(maxId):\n",
    "        binv[idx,pos] = 1\n",
    "\n",
    "    return binv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_labels = model.predict(X_test)\n",
    "\n",
    "predicted = binarize(y_test_predicted_labels)\n",
    "\n",
    "print(\"predicted shape \", y_test_predicted_labels.shape)\n",
    "print(\"y_test    shape \", Y_test.shape)\n",
    "print(\"predicted       \", y_test_predicted_labels)\n",
    "print(\"predicted bin.  \", predicted)\n",
    "print(\"Y test          \", Y_test)\n",
    "\n",
    "\n",
    "accuracy_score(Y_test, predicted)\n",
    "avg='weighted'\n",
    "print('f1_score  :', f1_score(Y_test, predicted, average = avg))\n",
    "#print('precision :', average_precision_score(y_test, predicted, average = avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = array(\n",
    "    [[0.2226171,  0.29442668, 0.05272774, 0.25584626, 0.22210537],\n",
    "     [0.22258776, 0.219336527, 0.05255532, 0.35723976, 0.22122917]])\n",
    "maxId = np.argmax(t, axis=1)\n",
    "print(maxId)\n",
    "print(t.shape)\n",
    "binv = np.zeros((t.shape))\n",
    "for idx, pos in enumerate(maxId):\n",
    "    binv[idx,pos] = 1\n",
    "\n",
    "print(binv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X shape', X_train.shape)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
