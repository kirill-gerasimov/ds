# Data Stuff
See notebooks in the `nb` dir. Some extras:

1. Intro
   * A [J.P. Morgan report](https://www.jpmorgan.com/global/research/machine-learning) on ML in banking incl. some experiments of their own
2. Iris dataset exploration
   * <https://machinelearningmastery.com/machine-learning-in-python-step-by-step/>
   * <http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html>
3. Credit fraud detection
   * <https://www.kaggle.com/mlg-ulb/creditcardfraud>
4. Text classification, Reuters21578 news dataset
   * [Intro slides from Stanford's Dan Jurafsky](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf)
   * [Reuters21578 news dataset](https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection) (also as a [CSV](https://raw.githubusercontent.com/ZhibingChen/DM-Reuters21578/master/reutersCSV.csv))
5. Pandas and Numpy [Intro / Cheatsheets](https://github.com/kirill-gerasimov/ds/tree/master/nb/5_pandas_introduction)

6, 7. Logistic regression, decision trees, boosting, hyperparameter tuning -- discussed offline

8a. SVM -- a brief [comparison of kernels](https://github.com/kirill-gerasimov/ds/blob/master/nb/8_topic_modeling_and_svm/Compare%20SVM%20Kernels%20By%20Olga%20Odintsova.html)

8b. Topic modeling [applied to get extra features](https://github.com/kirill-gerasimov/ds/blob/master/nb/8_topic_modeling_and_svm/reuters21578_topic-modeling.ipynb)

9a. [Some experiments with word2vec](https://github.com/kirill-gerasimov/ds/tree/master/nb/9_word2vec_and_nn) incl. a word analogies demo and text classification

9b. URLs to some curious resources about neural networks

### Resources
1. Vorontsov / Yandex / MIPT
   * [intro course](https://www.coursera.org/learn/vvedenie-mashinnoe-obuchenie/), short simple videos (but with homework takes 7w)
   * [specialization](https://www.coursera.org/specializations/machine-learning-data-analysis) based on the previous course
   * the [YDS course](https://yandexdataschool.ru/edu-process/courses/machine-learning) both above are actually based on
   * [its wiki page](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29) with slides and notes
2. Natural language processing [overview course](https://www.coursera.org/learn/language-processing/)
3. [Andrew Ng](https://www.coursera.org/learn/machine-learning)
4. [ODS community course](https://habr.com/company/ods/blog/322626/)

### Methods

1. classifiers: logreg, MLP, kNN, SVM, decision trees, RF, gradient boosting, naive Bayes
2. dimensionality reduction, visualization: t-SNE (vis. only), PCA, UMAP
3. text pre-processing: stemming / lemmatization, bag of words approach, TF-IDF
4. topic modeling (SVD, LDA)
5. word vector representations (word2vec, GloVe, fastText)
6. sentence / document embeddings (SIF, doc2vec, StarSpace)
7. advanced models: LSTM, GRU, CNN, ELMo, ULMFiT, Transformer

### Tools

1. python, numpy, scipy, pandas, matplotlib, seaborn, jupyter -- all the basics
2. scikit-learn -- classical algorithms
3. tensorflow -- neural networks and more
4. gensim -- topic modeling
5. nltk -- text processing
6. spaCy -- advanced natural language processing

